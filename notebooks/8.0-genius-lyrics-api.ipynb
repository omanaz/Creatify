{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "\n",
    "genius = lyricsgenius.Genius('9dKJP0ay75Pm4xmOvUUCzejH-ruwxF4fhmy90B9QnNzKCMuiL6GaFNFo87P26twa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import logging\n",
    "import spotipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from spotipy import oauth2\n",
    "import re\n",
    "\n",
    "import credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/lc5f5cgd1m5_ghmjddlz8r600000gn/T/ipykernel_22145/2781803620.py:7: DeprecationWarning: You're using 'as_dict = True'.get_access_token will return the token string directly in future versions. Please adjust your code accordingly, or use get_cached_token instead.\n",
      "  token = sp_oauth.get_access_token(code)\n"
     ]
    }
   ],
   "source": [
    "# Gain access to mainupation\n",
    "SCOPE = ('user-read-recently-played,user-library-read,user-read-currently-playing,playlist-read-private,playlist-modify-private,playlist-modify-public,user-read-email,user-modify-playback-state,user-read-private,user-read-playback-state')\n",
    "sp_oauth = oauth2.SpotifyOAuth(credentials.SPOTIPY_CLIENT_ID,credentials.SPOTIPY_CLIENT_SECRET, credentials.SPOTIPY_REDIRECT_URI ,scope=SCOPE )\n",
    "\n",
    "#click \"Accept\" in your browser when the auth window pops up\n",
    "code = sp_oauth.get_auth_response(open_browser=True)\n",
    "token = sp_oauth.get_access_token(code)\n",
    "refresh_token = token['refresh_token']\n",
    "sp = spotipy.Spotify(auth=token['access_token'])\n",
    "username = sp.current_user()['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_info = sp.track('6rEc69BeR8xHy3kN1ZFCsX')\n",
    "\n",
    "# Extract the track name and artist name from the track info\n",
    "track_name = track_info['name']\n",
    "artist_name = track_info['artists'][0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Selfish\" by Tensnake...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "song = genius.search_song(track_name, artist=artist_name)\n",
    "lyrics = song.lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selfish LyricsThere was a time when it was all about us, nobody more\n",
      "Seems like every other night you're out that door\n",
      "And I don't know what you're doing, maybe pursuing another man\n",
      "There was a time when I'd lay it all down on the line\n",
      "And bare everything I had til you were mine\n",
      "But now all bets are off, can't afford the cost of loving you\n",
      "\n",
      "Why you so selfish? Be by my side\n",
      "I'll runaway and hide til I can't help it\n",
      "I know you're not here with me\n",
      "Just how much more giving can I take?\n",
      "There was a time when you didn't care at all if you were poor\n",
      "Now all you seem to want is to have more\n",
      "And the cash you keep on spending\n",
      "It could be mending our broken lives\n",
      "Back in the day, before all you cared about was yourself\n",
      "Your concerns were more for love and heart and health\n",
      "But I won't stick a brighter day\n",
      "And the only cure for me is your love\n",
      "\n",
      "Why you so selfish?\n",
      "Be by my side, I'll runaway and hide til I can't help it\n",
      "I know you're not here with me\n",
      "Just how much more giving can I take?\n",
      "\n",
      "Take it, take it all, take it, you'll take it all (x4)\n",
      "Why are you so selfish?You might also likeEmbed\n"
     ]
    }
   ],
   "source": [
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding in NLP Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_lyrics(lyrics):\n",
    "    # Tokenize the lyrics into sentences\n",
    "    sentences = sent_tokenize(lyrics)\n",
    "    \n",
    "    # Tokenize each sentence into words\n",
    "    words = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Remove stop words and stem the remaining words\n",
    "    stemmed_words = []\n",
    "    for sentence in words:\n",
    "        for word in sentence:\n",
    "            if word.lower() not in stop_words:\n",
    "                stemmed_word = stemmer.stem(word)\n",
    "                stemmed_words.append(stemmed_word)\n",
    "    \n",
    "    # Combine the stemmed words into a single string\n",
    "    preprocessed_lyrics = ' '.join(stemmed_words)\n",
    "    \n",
    "    return preprocessed_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Selfish', 'LyricsThere', 'was', 'a', 'time', 'when', 'it', 'was', 'all', 'about', 'us', ',', 'nobody', 'more', 'Seems', 'like', 'every', 'other', 'night', 'you', \"'re\", 'out', 'that', 'door', 'And', 'I', 'do', \"n't\", 'know', 'what', 'you', \"'re\", 'doing', ',', 'maybe', 'pursuing', 'another', 'man', 'There', 'was', 'a', 'time', 'when', 'I', \"'d\", 'lay', 'it', 'all', 'down', 'on', 'the', 'line', 'And', 'bare', 'everything', 'I', 'had', 'til', 'you', 'were', 'mine', 'But', 'now', 'all', 'bets', 'are', 'off', ',', 'ca', \"n't\", 'afford', 'the', 'cost', 'of', 'loving', 'you', 'Why', 'you', 'so', 'selfish', '?', 'Be', 'by', 'my', 'side', 'I', \"'ll\", 'runaway', 'and', 'hide', 'til', 'I', 'ca', \"n't\", 'help', 'it', 'I', 'know', 'you', \"'re\", 'not', 'here', 'with', 'me', 'Just', 'how', 'much', 'more', 'giving', 'can', 'I', 'take', '?', 'There', 'was', 'a', 'time', 'when', 'you', 'did', \"n't\", 'care', 'at', 'all', 'if', 'you', 'were', 'poor', 'Now', 'all', 'you', 'seem', 'to', 'want', 'is', 'to', 'have', 'more', 'And', 'the', 'cash', 'you', 'keep', 'on', 'spending', 'It', 'could', 'be', 'mending', 'our', 'broken', 'lives', 'Back', 'in', 'the', 'day', ',', 'before', 'all', 'you', 'cared', 'about', 'was', 'yourself', 'Your', 'concerns', 'were', 'more', 'for', 'love', 'and', 'heart', 'and', 'health', 'But', 'I', 'wo', \"n't\", 'stick', 'a', 'brighter', 'day', 'And', 'the', 'only', 'cure', 'for', 'me', 'is', 'your', 'love', 'Why', 'you', 'so', 'selfish', '?', 'Be', 'by', 'my', 'side', ',', 'I', \"'ll\", 'runaway', 'and', 'hide', 'til', 'I', 'ca', \"n't\", 'help', 'it', 'I', 'know', 'you', \"'re\", 'not', 'here', 'with', 'me', 'Just', 'how', 'much', 'more', 'giving', 'can', 'I', 'take', '?', 'Take', 'it', ',', 'take', 'it', 'all', ',', 'take', 'it', ',', 'you', \"'ll\", 'take', 'it', 'all', '(', 'x4', ')', 'Why', 'are', 'you', 'so', 'selfish', '?', 'You', 'might', 'also', 'likeEmbed']\n"
     ]
    }
   ],
   "source": [
    "import svgling\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(lyrics)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Remove stop words from each string\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens)):\n\u001b[1;32m      9\u001b[0m     \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Remove stop words from the list of words\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mcasefold() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Join the filtered words back into a string and replace the original string\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     tokens[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_words)\n",
      "Cell \u001b[0;32mIn[62], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Remove stop words from each string\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens)):\n\u001b[1;32m      9\u001b[0m     \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Remove stop words from the list of words\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mcasefold() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Join the filtered words back into a string and replace the original string\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     tokens[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_words)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from each string\n",
    "for i in range(len(tokens)):\n",
    "    \n",
    "    # Remove stop words from the list of words\n",
    "    filtered_words = [word for word in tokens if word.casefold() not in stop_words]\n",
    "    \n",
    "    # Join the filtered words back into a string and replace the original string\n",
    "    tokens[i] = ' '.join(filtered_words)\n",
    "\n",
    "    # Remove any resulting empty strings\n",
    "    if tokens[i] == '':\n",
    "        tokens.pop(i)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.08719135802469136, subjectivity=0.5785493827160493)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "textblob_lyrics = TextBlob(lyrics)\n",
    "textblob_lyrics.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 8), (\"n't\", 6), ('?', 5), ('take', 5), (\"'re\", 4), ('time', 3), ('know', 3), ('til', 3), ('ca', 3), ('selfish', 3)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Combine the preprocessed lyrics from all the songs into a single string\n",
    "all_lyrics = ' '.join(tokens)\n",
    "\n",
    "# Tokenize the lyrics into individual words\n",
    "all_words = word_tokenize(all_lyrics)\n",
    "\n",
    "# Calculate the frequency distribution of the words\n",
    "freq_dist = FreqDist(all_words)\n",
    "\n",
    "# Get the 10 most common words\n",
    "most_common_words = freq_dist.most_common(10)\n",
    "\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('cse217a')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e27b9380ff38cc7a9bd155aef51d2581199facd5a3adc549696474b8eacd75fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
